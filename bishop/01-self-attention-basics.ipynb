{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X = \n",
      "[[52 48 16]\n",
      " [ 3  5  7]\n",
      " [ 3  5  7]]\n",
      "\n",
      "XX^T = \n",
      "[[5264  508  508]\n",
      " [ 508   83   83]\n",
      " [ 508   83   83]]\n",
      "\n",
      "exp(XX^T) = \n",
      "[[0.00000000e+000 2.39002912e-221 2.39002912e-221]\n",
      " [2.39002912e-221 8.98582594e-037 8.98582594e-037]\n",
      " [2.39002912e-221 8.98582594e-037 8.98582594e-037]]\n",
      "\n",
      "sum(exp(XX^T)) = \n",
      "[4.78005825e-221 1.79716519e-036 1.79716519e-036]\n",
      "\n",
      "Attention Weights Softmax[XX^T] = \n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n",
      "\n",
      "Self Attention, Y =\n",
      "[[ 0.  0.  0.]\n",
      " [29. 29. 15.]\n",
      " [29. 29. 15.]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    exp_num = np.exp(-X)\n",
    "\n",
    "    print('exp(XX^T) = ')\n",
    "    # print(np.round(exp_num, 2))\n",
    "    print(exp_num)\n",
    "    print('')\n",
    "\n",
    "    exp_sum = np.sum(exp_num, axis=1)\n",
    "    \n",
    "    print('sum(exp(XX^T)) = ')\n",
    "    # print(np.round(exp_sum, 2))\n",
    "    print(exp_sum)\n",
    "    print('')\n",
    "    \n",
    "    return np.nan_to_num(exp_num / exp_sum, nan=0)\n",
    "\n",
    "def rand_ints(size):\n",
    "    return np.random.randint(0,9, size)\n",
    "\n",
    "X = np.array([\n",
    "    np.random.randint(1, 100, 3),\n",
    "    np.array([3,5,7]),               # Should be similar wrt row 4\n",
    "    np.array([3,5,7]),               # Should be similar wrt row 1\n",
    "])\n",
    "\n",
    "print('Input X = ')\n",
    "print(X)\n",
    "print('')\n",
    "\n",
    "print('XX^T = ')\n",
    "print(X @ X.T)\n",
    "print('')\n",
    "\n",
    "\n",
    "atten_weights = softmax(X @ X.T)\n",
    "\n",
    "print('Attention Weights Softmax[XX^T] = ')\n",
    "print(np.round(atten_weights, 2))\n",
    "\n",
    "print('')\n",
    "print('Self Attention, Y =')\n",
    "print(np.round(atten_weights @ X, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X = \n",
      "[[  1   2   3   4   5]\n",
      " [  1   2   3   4   5]\n",
      " [ 42  48  28  57  67]\n",
      " [905 108 273 378 796]\n",
      " [354 226 251 492 247]]\n",
      "\n",
      "XX^T = \n",
      "[[     55      55     785    7432    4762]\n",
      " [     55      55     785    7432    4762]\n",
      " [    785     785   12590  125716   77337]\n",
      " [   7432    7432  125716 1681718  795889]\n",
      " [   4762    4762   77337  795889  542466]]\n",
      "\n",
      "Attention Weights Softmax[XX^T] = \n",
      "exp(XX^T) = \n",
      "[[1.29958143e-24 1.29958143e-24 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.29958143e-24 1.29958143e-24 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]]\n",
      "\n",
      "sum(exp(XX^T)) = \n",
      "[2.59916285e-24 2.59916285e-24 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00]\n",
      "\n",
      "[[0.5 0.5 0.  0.  0. ]\n",
      " [0.5 0.5 0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "\n",
      "Self Attention, Y =\n",
      "[[1. 2. 3. 4. 5.]\n",
      " [1. 2. 3. 4. 5.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1j/chhd52wn0z97rq9skbstg0_00000gn/T/ipykernel_2932/1564840644.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.nan_to_num(exp_num / exp_sum, nan=0)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    np.arange(1,6),\n",
    "    np.arange(1,6),               # Should be similar wrt row 4\n",
    "    np.random.randint(0, 100, 5),\n",
    "    np.random.randint(50, 1000, 5),\n",
    "    np.random.randint(80, 500, 5),\n",
    "])\n",
    "\n",
    "print('X = ')\n",
    "print(X)\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "print('XX^T = ')\n",
    "print(X @ X.T)\n",
    "print('')\n",
    "\n",
    "\n",
    "print('Attention Weights Softmax[XX^T] = ')\n",
    "atten_weights = softmax(X @ X.T)\n",
    "print(np.round(atten_weights, 2))\n",
    "\n",
    "print('')\n",
    "print('Self Attention, Y =')\n",
    "print(np.round(atten_weights @ X, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
