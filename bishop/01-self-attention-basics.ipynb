{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(matrix):\n",
    "    # Subtract the max for numerical stability\n",
    "    shift_matrix = matrix - np.max(matrix, axis=1, keepdims=True)\n",
    "    exp_matrix = np.exp(shift_matrix)\n",
    "    sum_exp = np.sum(exp_matrix, axis=1, keepdims=True)\n",
    "    softmax_matrix = exp_matrix / sum_exp\n",
    "    return np.round(softmax_matrix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0000000e-01 1.7212387e-14 5.0000000e-01]\n",
      " [4.5395809e-05 9.9990916e-01 4.5395809e-05]\n",
      " [5.0000000e-01 1.7212387e-14 5.0000000e-01]]\n",
      "[[5.00000000e-01 0.00000000e+00 5.00000000e-01]\n",
      " [4.53958000e-05 9.99909208e-01 4.53958000e-05]\n",
      " [5.00000000e-01 0.00000000e+00 5.00000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    np.array([3,5,7]),               # Should be similar wrt row 4\n",
    "    np.random.randint(1, 10, 3),\n",
    "    np.array([3,5,7]),               # Should be similar wrt row 1\n",
    "])\n",
    "\n",
    "print(jax.nn.softmax(X @ X.T))\n",
    "print(softmax(X @ X.T))\n",
    "# print(jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.72835065,  1.05970779,  1.4835909 ],\n",
       "       [11.020961  , 25.34914293, 11.020961  ],\n",
       "       [ 0.63582467,  1.05970779,  4.03281819]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.eye(3)) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X = \n",
      "[[7 8 4]\n",
      " [3 5 7]\n",
      " [3 5 7]]\n",
      "\n",
      "X^T = \n",
      "[[7 3 3]\n",
      " [8 5 5]\n",
      " [4 7 7]]\n",
      "\n",
      "XX^T = \n",
      "[[129  89  89]\n",
      " [ 89  83  83]\n",
      " [ 89  83  83]]\n",
      "\n",
      "Attention Weights Softmax[XX^T] = \n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "Self Attention, Y =\n",
      "[[7.   8.   4.  ]\n",
      " [6.98 7.99 4.01]\n",
      " [6.98 7.99 4.01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rand_ints(size):\n",
    "    return np.random.randint(0,9, size)\n",
    "\n",
    "X = np.array([\n",
    "    np.random.randint(1, 10, 3),\n",
    "    np.array([3,5,7]),               # Should be similar wrt row 4\n",
    "    np.array([3,5,7]),               # Should be similar wrt row 1\n",
    "])\n",
    "\n",
    "print('Input X = ')\n",
    "print(X)\n",
    "print('')\n",
    "\n",
    "print('X^T = ')\n",
    "print(X.T)\n",
    "print('')\n",
    "\n",
    "print('XX^T = ')\n",
    "print(X @ X.T)\n",
    "print('')\n",
    "\n",
    "\n",
    "atten_weights = softmax(X @ X.T)\n",
    "\n",
    "print('Attention Weights Softmax[XX^T] = ')\n",
    "print(np.round(atten_weights, 2))\n",
    "\n",
    "print('')\n",
    "print('Self Attention, Y =')\n",
    "print(np.round(atten_weights @ X, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "[[  1   2   3   4   5]\n",
      " [  1   2   3   4   5]\n",
      " [ 35  65   1  42  99]\n",
      " [910  70 894 638 916]\n",
      " [101 116 488 125 192]]\n",
      "\n",
      "X^T = \n",
      "[[  1   1  35 910 101]\n",
      " [  2   2  65  70 116]\n",
      " [  3   3   1 894 488]\n",
      " [  4   4  42 638 125]\n",
      " [  5   5  99 916 192]]\n",
      "\n",
      "XX^T = \n",
      "[[     55      55     831   10864    3257]\n",
      " [     55      55     831   10864    3257]\n",
      " [    831     831   17016  154774   35821]\n",
      " [  10864   10864  154774 2878336  791924]\n",
      " [   3257    3257   35821  791924  314290]]\n",
      "\n",
      "Attention Weights Softmax[XX^T] = \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ArrayImpl' object has no attribute 'tonumpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttention Weights Softmax[XX^T] = \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# atten_weights = softmax(X @ X.T)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m atten_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtonumpy\u001b[49m()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(atten_weights, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ArrayImpl' object has no attribute 'tonumpy'"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    np.arange(1,6),\n",
    "    np.arange(1,6),               # Should be similar wrt row 4\n",
    "    np.random.randint(0, 100, 5),\n",
    "    np.random.randint(50, 1000, 5),\n",
    "    np.random.randint(80, 500, 5),\n",
    "])\n",
    "# X = np.array([\n",
    "#     np.arange(1,6),\n",
    "#     np.arange(1,6),               # Should be similar wrt row 4\n",
    "#     np.random.randint(0, 10, 5),\n",
    "#     np.random.randint(10, 20, 5),\n",
    "#     np.random.randint(10, 30, 5),\n",
    "# ])\n",
    "\n",
    "print('X = ')\n",
    "print(X)\n",
    "print('')\n",
    "\n",
    "print('X^T = ')\n",
    "print(X.T)\n",
    "print('')\n",
    "\n",
    "\n",
    "print('XX^T = ')\n",
    "print(X @ X.T)\n",
    "print('')\n",
    "\n",
    "\n",
    "print('Attention Weights Softmax[XX^T] = ')\n",
    "# atten_weights = softmax(X @ X.T)\n",
    "atten_weights = jax.nn.softmax(X @ X.T).numpy()\n",
    "print(np.round(atten_weights, 2))\n",
    "\n",
    "print('')\n",
    "print('Self Attention, Y =')\n",
    "print(np.round(atten_weights @ X, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
